{"average_acc": 0.717948717948718, "subject_acc": {"lima_testset_single_turn_classify_parse_res_select_need_knowledge_gen_parse_res_test_gen_normalize": 0.6959247648902821, "vicuna_testset_single_turn_classify_parse_res_select_need_knowledge_gen_parse_res_test_gen_normalize": 0.7352941176470589, "wizardlm_testset_single_turn_classify_parse_res_select_need_knowledge_gen_parse_res_test_gen_normalize": 0.7391304347826086}}